{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a1cc7aa-b5b7-4755-8d92-98377f1affdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import vit_b_16, vit_l_16\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from einops import rearrange, repeat\n",
    "from x_transformers import Encoder, Decoder\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    "    ModelSummary,\n",
    ")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "429100ae-8c2c-456c-aeb2-c7c7bda85468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    \"\"\"Image to Patch Embedding\"\"\"\n",
    "\n",
    "    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=64):\n",
    "        super().__init__()\n",
    "        if isinstance(img_size, int):\n",
    "            img_size = img_size, img_size\n",
    "        if isinstance(patch_size, int):\n",
    "            patch_size = patch_size, patch_size\n",
    "        #calculate the number of patches\n",
    "        self.patch_shape = (img_size[0] // patch_size[0], img_size[1] // patch_size[1])\n",
    "\n",
    "        #convolutional layer to convert the image into patches\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_chans, embed_dim, kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        #flatten the patches\n",
    "        x = rearrange(x, 'b e h w -> b (h w) e')\n",
    "        return x\n",
    "\n",
    "\n",
    "# class Predictor(nn.Module):\n",
    "#     def __init__(self, embed_dim, num_heads, depth):\n",
    "#         super().__init__()\n",
    "        \n",
    "#         self.predictor = Decoder(dim = embed_dim, depth = depth, heads = num_heads)\n",
    "#     def forward(self, context_encoding, target_masks):\n",
    "#         x = torch.cat((context_encoding, target_masks), dim = 1)\n",
    "#         x = self.predictor(x)\n",
    "#         #return last len(target_masks) tokens\n",
    "#         l = x.shape[1]\n",
    "#         return x[:, l - target_masks.shape[1]:, :]\n",
    "\n",
    "\n",
    "class Predictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Recurrent predictor network to predict future representations\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim=1024, action_dim=2):\n",
    "        super(Predictor, self).__init__()\n",
    "        self.rnn = nn.GRUCell(representation_dim + action_dim, representation_dim)\n",
    "    \n",
    "    def forward(self, prev_rep, action):\n",
    "        # Concatenate previous representation and action\n",
    "        input_combined = torch.cat([prev_rep, action], dim=1)\n",
    "        return self.rnn(input_combined, prev_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0057392-2a4c-42a2-b6be-7e7b51178861",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JEPA(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_chans, embed_dim, enc_depth, pred_depth, num_heads, post_emb_norm=False, M = 4, mode=\"train\", layer_dropout=0.):\n",
    "        super().__init__()\n",
    "        self.M = M\n",
    "        self.mode = mode\n",
    "        self.layer_dropout = layer_dropout\n",
    "\n",
    "        #define the patch embedding and positional embedding\n",
    "        self.patch_embed = PatchEmbed(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)\n",
    "        self.patch_dim  = (self.patch_embed.patch_shape[0], self.patch_embed.patch_shape[1])\n",
    "        self.num_tokens = self.patch_embed.patch_shape[0] * self.patch_embed.patch_shape[1]\n",
    "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_tokens, embed_dim))\n",
    "\n",
    "        #define the cls and mask tokens\n",
    "        self.mask_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        nn.init.trunc_normal_(self.mask_token, 0.02)\n",
    "\n",
    "        print(self.mask_token.size())\n",
    "\n",
    "        #define the encoder and decoder, as well as the layer normalization and dropout\n",
    "        self.post_emb_norm = nn.LayerNorm(embed_dim) if post_emb_norm else nn.Identity()\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.teacher_encoder = Encoder(\n",
    "            dim=embed_dim,\n",
    "            heads=num_heads,\n",
    "            depth=enc_depth, \n",
    "            layer_dropout=self.layer_dropout,\n",
    "        )  \n",
    "        # self.student_encoder = copy.deepcopy(self.teacher_encoder).cuda()\n",
    "        self.student_encoder = copy.deepcopy(self.teacher_encoder)\n",
    "        self.predictor = Predictor(embed_dim)\n",
    "\n",
    "    @torch.no_grad() \n",
    "    def get_target_block(self, target_encoder, x, patch_dim, aspect_ratio, scale, M):  \n",
    "        #get the target block\n",
    "        target_encoder = target_encoder.eval()\n",
    "        x = target_encoder(x)\n",
    "        x = self.norm(x)\n",
    "        #get the patch dimensions\n",
    "        patch_h, patch_w = patch_dim\n",
    "        #get the number of patches\n",
    "        num_patches = patch_h * patch_w\n",
    "        #get the number of patches in the target block\n",
    "        num_patches_block = int(patch_h * patch_w * scale)\n",
    "        #get the height and width of the target block with aspect ratio\n",
    "        block_h = int(torch.sqrt(torch.tensor(num_patches_block / aspect_ratio)))\n",
    "        block_w = int(aspect_ratio * block_h)\n",
    "        #get the patches in the target block\n",
    "        target_block = torch.zeros((M, x.shape[0], block_h*block_w, x.shape[2]))\n",
    "        target_patches = []\n",
    "        all_patches = []\n",
    "        for z in range(M):\n",
    "            #get the starting patch\n",
    "            start_patch_h = torch.randint(0, patch_h - block_h+1, (1,)).item()\n",
    "            start_patch_w = torch.randint(0, patch_w - block_w+1, (1,)).item()\n",
    "            start_patch = start_patch_h * patch_w + start_patch_w\n",
    "\n",
    "            patches = []\n",
    "            #get the patches in the target block\n",
    "            for i in range(block_h):\n",
    "                for j in range(block_w):\n",
    "                    patches.append(start_patch + i * patch_w + j)\n",
    "                    if start_patch + i * patch_w + j not in all_patches:\n",
    "                        all_patches.append(start_patch + i * patch_w + j)\n",
    "                    \n",
    "            #get the target block\n",
    "            target_patches.append(patches)\n",
    "            target_block[z] = x[:, patches, :]\n",
    "        # return target_block.cuda(), target_patches, all_patches\n",
    "        return target_block, target_patches, all_patches\n",
    "\n",
    "    def get_context_block(self, x, patch_dim, aspect_ratio, scale, target_patches):\n",
    "        patch_h, patch_w = patch_dim\n",
    "        #get the number of patches in the target block\n",
    "        num_patches_block = int(patch_h * patch_w * scale)\n",
    "        #get the height and width of the target block with aspect ratio\n",
    "        block_h = int(torch.sqrt(torch.tensor(num_patches_block / aspect_ratio)))\n",
    "        block_w = int(aspect_ratio * block_h)\n",
    "        #get the starting patch\n",
    "        start_patch_h = torch.randint(0, patch_h - block_h+1, (1,)).item()\n",
    "        start_patch_w = torch.randint(0, patch_w - block_w+1, (1,)).item()\n",
    "        start_patch = start_patch_h * patch_w + start_patch_w\n",
    "        #get the patches in the context_block\n",
    "        patches = []\n",
    "        for i in range(block_h):\n",
    "            for j in range(block_w):\n",
    "                if start_patch + i * patch_w + j not in target_patches: #remove the target patches\n",
    "                    patches.append(start_patch + i * patch_w + j)\n",
    "        return x[:, patches, :]\n",
    "    \n",
    "\n",
    "    def forward(self, x, y, target_aspect_ratio=1, target_scale=1, context_aspect_ratio=1, context_scale=1):\n",
    "        #get the patch embeddings\n",
    "        x = self.patch_embed(x)\n",
    "        y = self.patch_embed(y)\n",
    "        b, n, e = x.shape\n",
    "        #add the positional embeddings\n",
    "        x = x + self.pos_embedding\n",
    "        y = y + self.pos_embedding\n",
    "        #normalize the embeddings\n",
    "        x = self.post_emb_norm(x)\n",
    "        y = self.post_emb_norm(y)\n",
    "        #if mode is test, we get return full embedding:\n",
    "        if self.mode == 'test':\n",
    "            return self.student_encoder(x)\n",
    "        # #get target embeddings\n",
    "        target_blocks, target_patches, all_patches = self.get_target_block(self.teacher_encoder, y, self.patch_dim, target_aspect_ratio, target_scale, self.M)\n",
    "        m, b, n, e = target_blocks.shape\n",
    "        #get context embedding\n",
    "\n",
    "        context_block = self.get_context_block(x, self.patch_dim, context_aspect_ratio, context_scale, all_patches)\n",
    "        context_encoding = self.student_encoder(context_block)\n",
    "        context_encoding = self.norm(context_encoding)\n",
    "\n",
    "\n",
    "        # prediction_blocks = torch.zeros((m, b, n, e)).cuda()\n",
    "        prediction_blocks = torch.zeros((m, b, n, e))\n",
    "        #get the prediction blocks, predict each target block separately\n",
    "        for i in range(m):\n",
    "            target_masks = self.mask_token.repeat(b, n, 1)\n",
    "            target_pos_embedding = self.pos_embedding[:, target_patches[i], :]\n",
    "            target_masks = target_masks + target_pos_embedding\n",
    "            prediction_blocks[i] = self.predictor(context_encoding, target_masks)\n",
    "\n",
    "        return prediction_blocks, target_blocks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69524ed8-23b0-4b38-87c2-8528dea4e8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IJEPA(pl.LightningModule):\n",
    "    def __init__(\n",
    "            self,\n",
    "            img_size=224,\n",
    "            patch_size=16,\n",
    "            in_chans=3, \n",
    "            embed_dim=64,\n",
    "            enc_heads=8,\n",
    "            enc_depth=8,\n",
    "            decoder_depth=6,\n",
    "            lr=1e-6,\n",
    "            weight_decay=0.05,\n",
    "            target_aspect_ratio = (0.75,1.5),\n",
    "            target_scale = (0.15, .2),\n",
    "            context_aspect_ratio = 1,\n",
    "            context_scale = (0.85,1.0),\n",
    "            M = 4, #number of different target blocks\n",
    "            m=0.996, #momentum\n",
    "            m_start_end = (.996, 1.)\n",
    "\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        #define models\n",
    "        self.model = JEPA(img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim, \n",
    "                                enc_depth = enc_depth, num_heads=enc_heads, pred_depth=decoder_depth, M=M)\n",
    "\n",
    "        #define hyperparameters\n",
    "        self.M = M\n",
    "        self.lr = lr\n",
    "        self.weight_decay = weight_decay\n",
    "        self.m = m\n",
    "        self.target_aspect_ratio = target_aspect_ratio\n",
    "        self.target_scale = target_scale\n",
    "        self.context_aspect_ratio = context_aspect_ratio\n",
    "        self.context_scale = context_scale\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_size = patch_size\n",
    "        self.num_tokens = (img_size // patch_size) ** 2\n",
    "        self.m_start_end = m_start_end\n",
    "\n",
    "        #define loss\n",
    "        self.criterion = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x, target_aspect_ratio, target_scale, context_aspect_ratio, context_scale):\n",
    "        return self.model(x, target_aspect_ratio, target_scale, context_aspect_ratio, context_scale)\n",
    "    \n",
    "    '''Update momentum for teacher encoder'''\n",
    "    def update_momentum(self, m):\n",
    "        student_model = self.model.student_encoder.eval()\n",
    "        teacher_model = self.model.teacher_encoder.eval()\n",
    "        with torch.no_grad():\n",
    "            for student_param, teacher_param in zip(student_model.parameters(), teacher_model.parameters()):\n",
    "                teacher_param.data.mul_(other=m).add_(other=student_param.data, alpha=1 - m)\n",
    "\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        #generate random target and context aspect ratio and scale\n",
    "        target_aspect_ratio = np.random.uniform(self.target_aspect_ratio[0], self.target_aspect_ratio[1])\n",
    "        target_scale = np.random.uniform(self.target_scale[0], self.target_scale[1])\n",
    "        context_aspect_ratio = self.context_aspect_ratio\n",
    "        context_scale = np.random.uniform(self.context_scale[0], self.context_scale[1])\n",
    "\n",
    "        y_student, y_teacher = self(x, target_aspect_ratio, target_scale, context_aspect_ratio, context_scale)\n",
    "        loss = self.criterion(y_student, y_teacher)\n",
    "        self.log('train_loss', loss)\n",
    "                    \n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x = batch\n",
    "        target_aspect_ratio = np.random.uniform(self.target_aspect_ratio[0], self.target_aspect_ratio[1])\n",
    "        target_scale = np.random.uniform(self.target_scale[0], self.target_scale[1])\n",
    "        context_aspect_ratio = self.context_aspect_ratio\n",
    "        context_scale = np.random.uniform(self.context_scale[0], self.context_scale[1])\n",
    "\n",
    "        y_student, y_teacher = self(x, target_aspect_ratio, target_scale, context_aspect_ratio, context_scale)\n",
    "        loss = self.criterion(y_student, y_teacher)\n",
    "        self.log('val_loss', loss)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx):\n",
    "        target_aspect_ratio = np.random.uniform(self.target_aspect_ratio[0], self.target_aspect_ratio[1])\n",
    "        target_scale = np.random.uniform(self.target_scale[0], self.target_scale[1])\n",
    "        context_aspect_ratio = self.context_aspect_ratio\n",
    "        context_scale = 1\n",
    "        self.model.mode = \"test\"\n",
    "\n",
    "        return self(batch, target_aspect_ratio, target_scale, context_aspect_ratio, context_scale) #just get teacher embedding\n",
    "\n",
    "    def on_after_backward(self):\n",
    "        self.update_momentum(self.m)\n",
    "        self.m += (self.m_start_end[1] - self.m_start_end[0]) / self.trainer.estimated_stepping_batches\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(\n",
    "            optimizer,\n",
    "            max_lr=self.lr,\n",
    "            total_steps=self.trainer.estimated_stepping_batches,\n",
    "        )\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"step\",\n",
    "            },\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ec2b5aa-fe5c-445e-b343-fc6f8d8d6a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class WallSample(NamedTuple):\n",
    "    states: torch.Tensor\n",
    "    locations: torch.Tensor\n",
    "    actions: torch.Tensor\n",
    "\n",
    "\n",
    "class WallDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        probing=False,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.states = np.load(f\"{data_path}/states.npy\", mmap_mode=\"r\")\n",
    "        self.actions = np.load(f\"{data_path}/actions.npy\")\n",
    "\n",
    "        if probing:\n",
    "            self.locations = np.load(f\"{data_path}/locations.npy\")\n",
    "        else:\n",
    "            self.locations = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        states = torch.from_numpy(self.states[i]).float().to(self.device)\n",
    "        actions = torch.from_numpy(self.actions[i]).float().to(self.device)\n",
    "\n",
    "        if self.locations is not None:\n",
    "            locations = torch.from_numpy(self.locations[i]).float().to(self.device)\n",
    "        else:\n",
    "            locations = torch.empty(0).to(self.device)\n",
    "\n",
    "        return WallSample(states=states, locations=locations, actions=actions)\n",
    "\n",
    "\n",
    "def create_wall_dataloader(\n",
    "    data_path,\n",
    "    probing=False,\n",
    "    device=\"cuda\",\n",
    "    batch_size=64,\n",
    "    train=True,\n",
    "):\n",
    "    ds = WallDataset(\n",
    "        data_path=data_path,\n",
    "        probing=probing,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size,\n",
    "        shuffle=train,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e61b8e40-45ca-4291-9b89-7df0cd4784e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class WallSample(NamedTuple):\n",
    "    states: torch.Tensor\n",
    "    locations: torch.Tensor\n",
    "    actions: torch.Tensor\n",
    "\n",
    "\n",
    "class WallDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        probing=False,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.states = np.load(f\"{data_path}/states.npy\", mmap_mode=\"r\")\n",
    "        self.actions = np.load(f\"{data_path}/actions.npy\")\n",
    "\n",
    "        if probing:\n",
    "            self.locations = np.load(f\"{data_path}/locations.npy\")\n",
    "        else:\n",
    "            self.locations = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        states = torch.from_numpy(self.states[i]).float().to(self.device)\n",
    "        actions = torch.from_numpy(self.actions[i]).float().to(self.device)\n",
    "\n",
    "        if self.locations is not None:\n",
    "            locations = torch.from_numpy(self.locations[i]).float().to(self.device)\n",
    "        else:\n",
    "            locations = torch.empty(0).to(self.device)\n",
    "\n",
    "        return WallSample(states=states, locations=locations, actions=actions)\n",
    "\n",
    "\n",
    "def create_wall_dataloader(\n",
    "    data_path,\n",
    "    probing=False,\n",
    "    device=\"cuda\",\n",
    "    batch_size=64,\n",
    "    train=True,\n",
    "):\n",
    "    ds = WallDataset(\n",
    "        data_path=data_path,\n",
    "        probing=probing,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size,\n",
    "        shuffle=train,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40db7684-5584-4f05-9c9b-dfdc735559dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = create_wall_dataloader('./DL24FA/train', device='cpu', batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57acb42f-b056-425f-9956-358b1819b2fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 17, 2, 65, 65])\n",
      "torch.Size([1, 16, 2])\n",
      "torch.Size([1, 0])\n",
      "\n",
      "\n",
      "tensor([ 0.8981, -0.1178])\n",
      "tensor([ 0.6551, -0.4548])\n",
      "tensor([ 0.9138, -0.0485])\n",
      "tensor([0.5497, 0.3975])\n",
      "tensor([0.6934, 0.4740])\n",
      "tensor([0.7901, 0.2268])\n",
      "tensor([ 1.3507, -0.4438])\n",
      "tensor([ 1.1281, -0.1383])\n",
      "tensor([1.0353, 0.4285])\n",
      "tensor([ 1.0585, -0.4855])\n",
      "tensor([1.0278, 0.3916])\n",
      "tensor([ 0.8944, -0.4860])\n",
      "tensor([0.9172, 0.4508])\n",
      "tensor([0.5561, 0.2728])\n",
      "tensor([ 0.7411, -0.0362])\n",
      "tensor([0.8644, 0.0572])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/state/partition1/job-54276732/ipykernel_2817137/203106581.py:32: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647378361/work/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "  states = torch.from_numpy(self.states[i]).float().to(self.device)\n"
     ]
    }
   ],
   "source": [
    "for x in dataloader:\n",
    "    print(x.states.shape)\n",
    "    print(x.actions.shape)\n",
    "    print(x.locations.shape)\n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "    t1 = transforms.ToPILImage()\n",
    "    for y in range(16):\n",
    "        print(x.actions[0][y])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e875b0a-d621-4b1c-b216-8b04b5fef148",
   "metadata": {},
   "source": [
    "## New Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e7cfcc8-58f7-48a2-92e5-8f2167732327",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import vit_b_16, ViT_B_16_Weights\n",
    "\n",
    "class VitEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer (ViT) based encoder for custom input dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, representation_dim=512, input_channels=2):\n",
    "        super(VitEncoder, self).__init__()\n",
    "        # Custom initial convolutional layer to handle 2-channel input\n",
    "        self.input_adapter = nn.Conv2d(input_channels, 3, kernel_size=3, padding=1)\n",
    "        \n",
    "        # Load pre-trained ViT model\n",
    "        self.vit = vit_b_16(weights=ViT_B_16_Weights.IMAGENET1K_V1)\n",
    "        \n",
    "        # Remove the classification head\n",
    "        self.vit.heads = nn.Identity()\n",
    "        \n",
    "        # Add a projection head to get desired representation dimension\n",
    "        self.projection_head = nn.Sequential(\n",
    "            nn.Linear(self.vit.hidden_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, representation_dim)\n",
    "        )\n",
    "        \n",
    "        # Freeze base ViT weights\n",
    "        for param in self.vit.parameters():\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Adapt input to 3 channels\n",
    "        x = self.input_adapter(x)\n",
    "        \n",
    "        # Ensure input is compatible with ViT\n",
    "        x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        # Project the input through the convolutional layer\n",
    "        x = self.vit.conv_proj(x)  # Shape: (batch_size, hidden_dim, height, width)\n",
    "        \n",
    "        # Flatten spatial dimensions into a sequence\n",
    "        batch_size, hidden_dim, height, width = x.shape\n",
    "        x = x.flatten(2).transpose(1, 2)  # Shape: (batch_size, seq_length, hidden_dim)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.vit.encoder.pos_embedding[:, :x.size(1), :]\n",
    "        \n",
    "        # Pass through the encoder\n",
    "        features = self.vit.encoder.layers(self.vit.encoder.dropout(x))\n",
    "        features = self.vit.encoder.ln(features)\n",
    "        \n",
    "        # Take the [CLS] token embedding (first token)\n",
    "        cls_embedding = features[:, 0]\n",
    "        \n",
    "        # Project to desired representation dimension\n",
    "        return self.projection_head(cls_embedding)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class ActionAwarePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-layer predictor that considers both previous representation and action\n",
    "    \"\"\"\n",
    "    def __init__(self, representation_dim=512, action_dim=2):\n",
    "        super(ActionAwarePredictor, self).__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(representation_dim + action_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, representation_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, prev_rep, action):\n",
    "        # Concatenate previous representation and action\n",
    "        input_combined = torch.cat([prev_rep, action], dim=1)\n",
    "        return self.network(input_combined)\n",
    "\n",
    "class JEPAWorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Joint Embedding Predictive Architecture World Model with ViT\n",
    "    \"\"\"\n",
    "    def __init__(self, representation_dim=512, action_dim=2):\n",
    "        super(JEPAWorldModel, self).__init__()\n",
    "        self.encoder = VitEncoder(representation_dim)\n",
    "        self.predictor = ActionAwarePredictor(representation_dim, action_dim)\n",
    "        \n",
    "        # Use same encoder for target encoder (similar to VicReg)\n",
    "        self.target_encoder = VitEncoder(representation_dim)\n",
    "        \n",
    "        # Synchronize target encoder with main encoder\n",
    "        self.update_target_encoder()\n",
    "    \n",
    "    def update_target_encoder(self, tau=0.995):\n",
    "        \"\"\"\n",
    "        Exponential Moving Average (EMA) update of target encoder\n",
    "        \"\"\"\n",
    "        for param_q, param_k in zip(self.encoder.parameters(), self.target_encoder.parameters()):\n",
    "            param_k.data = param_k.data * tau + param_q.data * (1. - tau)\n",
    "    \n",
    "    def forward(self, observations, actions):\n",
    "        # Encode observations\n",
    "        encoded_states = [self.encoder(observations[:, 0])]\n",
    "        predicted_states = []\n",
    "        \n",
    "        # Predict future representations\n",
    "        for t in range(1, observations.shape[1]):\n",
    "            prev_state = encoded_states[-1]\n",
    "            curr_action = actions[:, t-1]\n",
    "            \n",
    "            # Predict next state\n",
    "            predicted_state = self.predictor(prev_state, curr_action)\n",
    "            predicted_states.append(predicted_state)\n",
    "            \n",
    "            # Encode current observation with target encoder\n",
    "            with torch.no_grad():\n",
    "                curr_encoded_state = self.target_encoder(observations[:, t])\n",
    "            encoded_states.append(curr_encoded_state)\n",
    "        \n",
    "        return predicted_states, encoded_states[1:]\n",
    "    \n",
    "    def compute_loss(self, predicted_states, target_states):\n",
    "        \"\"\"\n",
    "        Multi-objective loss to prevent representation collapse\n",
    "        \"\"\"\n",
    "        # 1. Prediction Loss: Minimize distance between predicted and target states\n",
    "        pred_loss = F.mse_loss(torch.stack(predicted_states), torch.stack(target_states))\n",
    "        \n",
    "        # 2. Variance Loss: Encourage representations to have non-zero variance\n",
    "        std_loss = self.variance_loss(predicted_states)\n",
    "        \n",
    "        # 3. Covariance Loss: Decorrelate representation dimensions\n",
    "        cov_loss = self.covariance_loss(predicted_states)\n",
    "        \n",
    "        # Weighted combination of losses\n",
    "        total_loss = pred_loss + 1e-4 * (std_loss + cov_loss)\n",
    "        return total_loss\n",
    "    \n",
    "    def variance_loss(self, representations, min_std=0.1):\n",
    "        \"\"\"Encourage each feature to have non-zero variance\"\"\"\n",
    "        repr_tensor = torch.stack(representations)\n",
    "        std_loss = torch.max(\n",
    "            torch.tensor(min_std), \n",
    "            torch.sqrt(repr_tensor.var(dim=0) + 1e-7)\n",
    "        ).mean()\n",
    "        return std_loss\n",
    "    \n",
    "    def covariance_loss(self, representations):\n",
    "        \"\"\"Decorrelate representation dimensions\"\"\"\n",
    "        repr_tensor = torch.stack(representations)\n",
    "        \n",
    "        # Center the representations\n",
    "        repr_tensor = repr_tensor - repr_tensor.mean(dim=0)\n",
    "        \n",
    "        # Flatten tensor (keep batch dimension intact)\n",
    "        repr_tensor = repr_tensor.view(repr_tensor.shape[0], -1)\n",
    "        \n",
    "        # Compute covariance matrix\n",
    "        cov_matrix = (repr_tensor.T @ repr_tensor) / (repr_tensor.shape[0] - 1)\n",
    "        \n",
    "        # Decorrelate dimensions (set diagonal to zero)\n",
    "        cov_matrix.fill_diagonal_(0)\n",
    "        \n",
    "        # Compute loss\n",
    "        cov_loss = (cov_matrix ** 2).sum()\n",
    "        return cov_loss\n",
    "\n",
    "class DataTransforms:\n",
    "    \"\"\"\n",
    "    Image augmentations and preprocessing for JEPA training\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def get_train_transforms():\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "def train_jepa_model(model, dataloader, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Training loop for JEPA world model\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch_observations = batch.states.to(device)\n",
    "        batch_actions = batch.actions.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predicted_states, target_states = model(batch_observations, batch_actions)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = model.compute_loss(predicted_states, target_states)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update target encoder (EMA)\n",
    "        model.update_target_encoder()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd523738-49fd-482d-bf63-0938771c0ece",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'JEPAWorldModel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m action_dim \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m jepa_model \u001b[38;5;241m=\u001b[39m \u001b[43mJEPAWorldModel\u001b[49m(\n\u001b[1;32m      7\u001b[0m     representation_dim\u001b[38;5;241m=\u001b[39mrepresentation_dim, \n\u001b[1;32m      8\u001b[0m     action_dim\u001b[38;5;241m=\u001b[39maction_dim\n\u001b[1;32m      9\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Optimizer\u001b[39;00m\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(jepa_model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-4\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'JEPAWorldModel' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "representation_dim = 1024\n",
    "action_dim = 2\n",
    "\n",
    "# Initialize model\n",
    "jepa_model = JEPAWorldModel(\n",
    "    representation_dim=representation_dim, \n",
    "    action_dim=action_dim\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(jepa_model.parameters(), lr=1e-4)\n",
    "\n",
    "# TODO: Implement actual data loading from /scratch/DL24FA/train\n",
    "# dataloader = ...\n",
    "dataloader = create_wall_dataloader('./DL24FA/train', device='cpu', batch_size=8)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_jepa_model(jepa_model, dataloader, optimizer, device, epoch)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Optional: Learning rate scheduling, model checkpointing\n",
    "    # scheduler.step()\n",
    "\n",
    "# Save model\n",
    "torch.save(jepa_model.state_dict(), \"jepa_vit_world_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f3d33a-3611-48c1-b985-3396edff97f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
