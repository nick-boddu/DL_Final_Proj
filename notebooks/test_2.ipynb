{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8e2a6ef-e2ed-4820-83d6-461feba1bae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models import vit_b_16, vit_l_16\n",
    "from torchvision.models.vision_transformer import VisionTransformer\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "from einops import rearrange, repeat\n",
    "from x_transformers import Encoder, Decoder\n",
    "\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.callbacks import (\n",
    "    ModelCheckpoint,\n",
    "    LearningRateMonitor,\n",
    "    ModelSummary,\n",
    ")\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb356a78-08eb-42bf-95cb-5484832224d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "representation_dim = 512\n",
    "action_dim = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba1bf54a-e5ed-4b6a-b72f-9b0242a01e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, in_channels, embed_dim):\n",
    "        super().__init__()\n",
    "        img_size, patch_size = (img_size, img_size), (patch_size, patch_size)\n",
    "        self.num_patches = (img_size[0] // patch_size[0]) * (\n",
    "            img_size[1] // patch_size[1])\n",
    "        self.conv = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.conv(X).flatten(2).transpose(1, 2)\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = AttentionUtil(0.2)\n",
    "        self.W_q = nn.LazyLinear(embed_dim)\n",
    "        self.W_k = nn.LazyLinear(embed_dim)\n",
    "        self.W_v = nn.LazyLinear(embed_dim)\n",
    "        self.W_o = nn.LazyLinear(embed_dim)\n",
    "    \n",
    "    def _transpose1(self, inp):\n",
    "        inp = inp.reshape(inp.shape[0], inp.shape[1], self.num_heads, -1)\n",
    "        inp = inp.permute(0, 2, 1, 3)\n",
    "        return inp.reshape(-1, inp.shape[2], inp.shape[3])\n",
    "    \n",
    "    def _transpose2(self, inp):\n",
    "        inp = inp.reshape(-1, self.num_heads, inp.shape[1], inp.shape[2])\n",
    "        inp = inp.permute(0, 2, 1, 3)\n",
    "        return inp.reshape(inp.shape[0], inp.shape[1], -1)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        queries = self._transpose1(self.W_q(queries))\n",
    "        keys = self._transpose1(self.W_k(keys))\n",
    "        values = self._transpose1(self.W_v(values))\n",
    "\n",
    "        output = self.attention(queries, keys, values)\n",
    "        output_concat = self._transpose2(output)\n",
    "        return self.W_o(output_concat)\n",
    "\n",
    "\n",
    "    \n",
    "class AttentionUtil(nn.Module):\n",
    "    def __init__(self, dropout):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, queries, keys, values):\n",
    "        d = queries.shape[-1]\n",
    "        scores = torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(d)\n",
    "        self.attention_weights = nn.functional.softmax(scores, dim=-1)\n",
    "        return torch.bmm(self.dropout(self.attention_weights), values)\n",
    "\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, mlp_dim, dropout):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        self.mlp = FCLayer(mlp_dim, embed_dim, dropout)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = X + self.attention(*([self.ln1(X)] * 3))\n",
    "        return X + self.mlp(self.ln2(X))\n",
    "\n",
    "    \n",
    "class FCLayer(nn.Module):\n",
    "    def __init__(self, mlp_num_hiddens, mlp_num_outputs, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.dense1 = nn.LazyLinear(mlp_num_hiddens)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dense2 = nn.LazyLinear(mlp_num_outputs)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.dropout2(self.dense2(self.dropout1(self.gelu(\n",
    "            self.dense1(x)))))\n",
    "\n",
    "\n",
    "class VisionTransformer_custom(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, in_channels, embed_dim, num_heads, mlp_dim,\n",
    "                 num_layers, num_classes, dropout):\n",
    "        super().__init__()\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            image_size, patch_size, in_channels, embed_dim)\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        num_steps = self.patch_embedding.num_patches + 1 \n",
    "        self.pos_embedding = nn.Parameter(\n",
    "            torch.randn(1, num_steps, embed_dim))\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.blks = nn.Sequential()\n",
    "        for i in range(num_layers):\n",
    "            self.blks.add_module(f\"{i}\", TransformerBlock(\n",
    "                embed_dim, num_heads, mlp_dim, dropout))\n",
    "        self.head = nn.Sequential(nn.LayerNorm(embed_dim),\n",
    "                                  nn.Linear(embed_dim, num_classes))\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.patch_embedding(X)\n",
    "        X = torch.cat((self.cls_token.expand(X.shape[0], -1, -1), X), 1)\n",
    "        X = self.dropout(X + self.pos_embedding)\n",
    "        for block in self.blks:\n",
    "            X = block(X)\n",
    "        return self.head(X[:, 0])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3232bf9-700a-4968-98f8-5758ecfa6801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         # self.vit = vit_b_16().to(device)\n",
    "#         self.vit = VisionTransformer(image_size=65, patch_size=13, mlp_dim=representation_dim, hidden_dim=representation_dim,\n",
    "#                                      num_layers=2, num_heads=2, dropout=0.2, num_classes=representation_dim).to(device)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x.to(device)\n",
    "#         # x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False) # [17, 2, 224, 224]\n",
    "        \n",
    "#         dummy_channel = torch.zeros(x.size(0), 1, x.size(2), x.size(3), device=device)\n",
    "#         x = torch.cat((x, dummy_channel), dim=1)  # Now [17, 3, 224, 224]\n",
    "#         print('x loc', x.device)\n",
    "        \n",
    "#         return self.vit(x)\n",
    "\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # self.vit = vit_b_16().to(device)\n",
    "        self.vit = VisionTransformer_custom(image_size=65, patch_size=13, in_channels=2,\n",
    "                                           embed_dim=representation_dim, num_heads=4, mlp_dim=representation_dim,\n",
    "                                           num_layers=4, num_classes=representation_dim, dropout=0.1).to(device)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        # x = F.interpolate(x, size=(224, 224), mode='bilinear', align_corners=False) # [17, 2, 224, 224]\n",
    "        \n",
    "        # dummy_channel = torch.zeros(x.size(0), 1, x.size(2), x.size(3), device=device)\n",
    "        # x = torch.cat((x, dummy_channel), dim=1)  # Now [17, 3, 224, 224]\n",
    "        # print('x loc', x.device)\n",
    "        \n",
    "        return self.vit(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e15c3d0-3327-4a09-84e1-f352d629d725",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor(nn.Module):\n",
    "    def __init__(self, representation_dim=representation_dim, action_dim=2):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(representation_dim + action_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(256),\n",
    "            nn.Linear(256, representation_dim)\n",
    "        ).to(device)\n",
    "    \n",
    "    def forward(self, prev_rep, action):\n",
    "        prev_rep, action = prev_rep.to(device), action.to(device)\n",
    "        # Concatenate previous representation and action\n",
    "        input_combined = torch.cat([prev_rep, action], dim=-1).to(device)\n",
    "        return self.network(input_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7b115f9d-dd26-4097-a936-550a2bd610fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class JEPAWorldModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Joint Embedding Predictive Architecture World Model with ViT\n",
    "    \"\"\"\n",
    "    def __init__(self, representation_dim=representation_dim, action_dim=2):\n",
    "        super().__init__()\n",
    "        self.encoder = VisionTransformer_custom(image_size=65, patch_size=13, in_channels=2,\n",
    "                                           embed_dim=representation_dim, num_heads=4, mlp_dim=representation_dim,\n",
    "                                           num_layers=4, num_classes=representation_dim, dropout=0.1).to(device)\n",
    "        self.predictor = Predictor(representation_dim, action_dim).to(device)\n",
    "        \n",
    "        # Use same encoder for target encoder (similar to VicReg)\n",
    "        self.target_encoder = VisionTransformer_custom(image_size=65, patch_size=13, in_channels=2,\n",
    "                                           embed_dim=representation_dim, num_heads=4, mlp_dim=representation_dim,\n",
    "                                           num_layers=4, num_classes=representation_dim, dropout=0.1).to(device)\n",
    "        \n",
    "        # Synchronize target encoder with main encoder\n",
    "        self.update_target_encoder()\n",
    "    \n",
    "    def update_target_encoder(self, tau=0.995):\n",
    "        \"\"\"\n",
    "        Exponential Moving Average (EMA) update of target encoder\n",
    "        \"\"\"\n",
    "        for param_q, param_k in zip(self.encoder.parameters(), self.target_encoder.parameters()):\n",
    "            param_k.data = param_k.data * tau + param_q.data * (1. - tau)\n",
    "    \n",
    "    # def forward(self, observations, actions):\n",
    "    #     # Encode observations\n",
    "    #     observations, actions = observations.to(device), actions.to(device)\n",
    "    #     encoded_states = [self.encoder(observations[:, 0])]\n",
    "    #     predicted_states = []\n",
    "    #     target_states = []\n",
    "        \n",
    "    #     # Predict future representations\n",
    "    #     for t in range(1, observations.shape[1]):\n",
    "    #         prev_state = encoded_states[-1]\n",
    "    #         curr_action = actions[:, t-1]\n",
    "\n",
    "    #         # Predict next state\n",
    "    #         predicted_state = self.predictor(prev_state, curr_action)\n",
    "    #         predicted_states.append(predicted_state)\n",
    "            \n",
    "    #         # Encode current observation with target encoder\n",
    "    #         with torch.no_grad():\n",
    "    #             curr_encoded_state = self.target_encoder(observations[:, t])\n",
    "    #         target_states.append(curr_encoded_state)\n",
    "\n",
    "    #         encoded_states.append(self.encoder(observations[:, t]))\n",
    "        \n",
    "    #     return predicted_states, target_states\n",
    "\n",
    "\n",
    "    def forward(self, observations, actions):\n",
    "        # Move observations and actions to device\n",
    "        observations, actions = observations.to(device), actions.to(device)\n",
    "                \n",
    "        # Encode all observations at once using the encoder\n",
    "        # encoded_all_states = self.encoder(observations.view(-1, *observations.shape[2:]))\n",
    "        batch_size, seq_len, channels, height, width = observations.shape\n",
    "        flat_observations = observations.view(-1, channels, height, width).to(device)\n",
    "        encoded_all_states = self.encoder(flat_observations).to(device)\n",
    "        encoded_all_states = encoded_all_states.view(*observations.shape[:2], -1).to(device)  # Reshape back to (batch, sequence, features)\n",
    "        \n",
    "        # Initialize storage for predicted and target states\n",
    "        predicted_states = []\n",
    "        target_states = []\n",
    "    \n",
    "        # Shift actions to align with the sequence (actions at t predict state at t+1)\n",
    "        prev_states = encoded_all_states[:, :-1]  # Remove the last state\n",
    "        next_states = encoded_all_states[:, 1:]   # Remove the first state\n",
    "        # curr_actions = actions[:, :-1]           # Align actions with prediction\n",
    "        \n",
    "        # Predict future representations in parallel\n",
    "        predicted_states = self.predictor(prev_states, actions).to(device)\n",
    "        \n",
    "        # Encode target states with target encoder\n",
    "        with torch.no_grad():\n",
    "            target_states = self.target_encoder(flat_observations).to(device)  # Skip the first observation for alignment\n",
    "            target_states = target_states.view(*observations.shape[:2], -1).to(device)\n",
    "            target_states = target_states[:, 1:]\n",
    "            target_states = target_states.to(device)\n",
    "        return predicted_states, target_states\n",
    "    \n",
    "    def compute_loss(self, predicted_states, target_states):\n",
    "        \"\"\"\n",
    "        Multi-objective loss to prevent representation collapse\n",
    "        \"\"\"\n",
    "        predicted_states, target_states = predicted_states.to(device), target_states.to(device)\n",
    "        # 1. Prediction Loss: Minimize distance between predicted and target states\n",
    "        # pred_loss = F.mse_loss(torch.stack(predicted_states), torch.stack(target_states))\n",
    "        pred_loss = F.mse_loss(predicted_states, target_states)\n",
    "        \n",
    "        # 2. Variance Loss: Encourage representations to have non-zero variance\n",
    "        std_loss = self.variance_loss(predicted_states)\n",
    "        \n",
    "        # 3. Covariance Loss: Decorrelate representation dimensions\n",
    "        cov_loss = self.covariance_loss(predicted_states)\n",
    "        \n",
    "        # Weighted combination of losses\n",
    "        total_loss = pred_loss + 1e-2 * (std_loss + cov_loss)\n",
    "        return total_loss\n",
    "    \n",
    "    def variance_loss(self, representations, min_std=0.1):\n",
    "        \"\"\"Encourage each feature to have non-zero variance\"\"\"\n",
    "        # repr_tensor = torch.stack(representations)\n",
    "        representations = representations.to(device)\n",
    "        std_loss = torch.relu(min_std - representations.std(dim=0)).mean()\n",
    "        return std_loss\n",
    "    \n",
    "    def covariance_loss(self, representations):\n",
    "        \"\"\"Decorrelate representation dimensions\"\"\"\n",
    "        # repr_tensor = torch.stack(representations)\n",
    "        representations = representations.to(device)\n",
    "        repr_tensor = representations\n",
    "        repr_tensor = repr_tensor.to(device)\n",
    "        \n",
    "        # Center the representations\n",
    "        repr_tensor = repr_tensor - repr_tensor.mean(dim=0)\n",
    "        \n",
    "        # Flatten tensor (keep batch dimension intact)\n",
    "        repr_tensor = repr_tensor.view(repr_tensor.shape[0], -1)\n",
    "        \n",
    "        # Compute covariance matrix\n",
    "        cov_matrix = (repr_tensor.T @ repr_tensor) / (repr_tensor.shape[0] - 1)\n",
    "        \n",
    "        # Decorrelate dimensions (set diagonal to zero)\n",
    "        cov_matrix.fill_diagonal_(0)\n",
    "        \n",
    "        # Compute loss\n",
    "        cov_loss = (cov_matrix ** 2).sum()\n",
    "        return cov_loss\n",
    "\n",
    "class DataTransforms:\n",
    "    \"\"\"\n",
    "    Image augmentations and preprocessing for JEPA training\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def get_train_transforms():\n",
    "        return transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "def train_jepa_model(model, dataloader, optimizer, device, epoch):\n",
    "    \"\"\"\n",
    "    Training loop for JEPA world model\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in dataloader:\n",
    "        batch_observations = batch.states.to(device)\n",
    "        batch_actions = batch.actions.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        predicted_states, target_states = model(batch_observations, batch_actions)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = model.compute_loss(predicted_states, target_states)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update target encoder (EMA)\n",
    "        model.update_target_encoder()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7e540bd0-bc56-4dd7-b756-d37c2f855d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Optional\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class WallSample(NamedTuple):\n",
    "    states: torch.Tensor\n",
    "    locations: torch.Tensor\n",
    "    actions: torch.Tensor\n",
    "\n",
    "\n",
    "class WallDataset:\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_path,\n",
    "        probing=False,\n",
    "        device=\"cuda\",\n",
    "    ):\n",
    "        self.device = device\n",
    "        self.states = np.load(f\"{data_path}/states.npy\", mmap_mode=\"r\")\n",
    "        self.actions = np.load(f\"{data_path}/actions.npy\")\n",
    "\n",
    "        if probing:\n",
    "            self.locations = np.load(f\"{data_path}/locations.npy\")\n",
    "        else:\n",
    "            self.locations = None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.states)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        states = torch.from_numpy(self.states[i]).float().to(self.device)\n",
    "        actions = torch.from_numpy(self.actions[i]).float().to(self.device)\n",
    "\n",
    "        if self.locations is not None:\n",
    "            locations = torch.from_numpy(self.locations[i]).float().to(self.device)\n",
    "        else:\n",
    "            locations = torch.empty(0).to(self.device)\n",
    "\n",
    "        return WallSample(states=states, locations=locations, actions=actions)\n",
    "\n",
    "\n",
    "def create_wall_dataloader(\n",
    "    data_path,\n",
    "    probing=False,\n",
    "    device=\"cuda\",\n",
    "    batch_size=64,\n",
    "    train=True,\n",
    "):\n",
    "    ds = WallDataset(\n",
    "        data_path=data_path,\n",
    "        probing=probing,\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size,\n",
    "        shuffle=train,\n",
    "        drop_last=True,\n",
    "        pin_memory=False,\n",
    "    )\n",
    "\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "39abae67-257f-4bc6-a686-8d573377ec70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch done\n"
     ]
    }
   ],
   "source": [
    "model = JEPAWorldModel(representation_dim=representation_dim, action_dim=2).to(device)\n",
    "dataloader = create_wall_dataloader('./DL24FA/train', device=device, batch_size=128)\n",
    "\n",
    "model2 = VisionTransformer_custom(image_size=65, patch_size=13, in_channels=2,\n",
    "                                           embed_dim=representation_dim, num_heads=4, mlp_dim=representation_dim,\n",
    "                                           num_layers=4, num_classes=representation_dim, dropout=0.1).to(device)\n",
    "\n",
    "\n",
    "for batch in dataloader:\n",
    "    pred, enc = model(batch.states, batch.actions)\n",
    "    print('batch done')\n",
    "    break\n",
    "    # observations, actions = batch.states, batch.actions\n",
    "    # batch_size, seq_len, channels, height, width = observations.shape\n",
    "    # flat_observations = observations.view(-1, channels, height, width).to(device)\n",
    "    # ans = model2(flat_observations)\n",
    "    # print('Pred shape', len(pred))\n",
    "    # print('enc shape', len(enc))\n",
    "    # print(pred[0].shape)\n",
    "    # print(enc[0].shape)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f25fc427-8b60-434b-8579-a46e6bb6e108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1148\n"
     ]
    }
   ],
   "source": [
    "print(len(dataloader))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d13a9e2a-8c24-4c90-b164-ed0e5e627194",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50649cae-8e96-4efc-b34b-11406fdfbc90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0838\n",
      "Epoch 2, Loss: 0.0173\n",
      "Epoch 3, Loss: 0.0160\n",
      "Epoch 4, Loss: 0.0151\n",
      "Epoch 5, Loss: 0.0145\n",
      "Epoch 6, Loss: 0.0146\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "jepa_model = JEPAWorldModel(\n",
    "    representation_dim=representation_dim, \n",
    "    action_dim=action_dim\n",
    ").to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(jepa_model.parameters(), lr=1e-4)\n",
    "\n",
    "# TODO: Implement actual data loading from /scratch/DL24FA/train\n",
    "# dataloader = ...\n",
    "dataloader = create_wall_dataloader('./DL24FA/train', device=device, batch_size=64)\n",
    "\n",
    "jepa_model.target_encoder.load_state_dict(jepa_model.encoder.state_dict())\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    avg_loss = train_jepa_model(jepa_model, dataloader, optimizer, device, epoch)\n",
    "    print(f\"Epoch {epoch+1}, Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Optional: Learning rate scheduling, model checkpointing\n",
    "    # scheduler.step()\n",
    "    torch.save(jepa_model.state_dict(), \"jepa_vit_world_model.pth\")\n",
    "\n",
    "# Save model\n",
    "torch.save(jepa_model.state_dict(), \"jepa_vit_world_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fae7f80-230c-4bee-bba7-8ca0d20636b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
